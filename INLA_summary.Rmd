---
title: "Example spatio-temporal INLA"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, fig.dim=c(4.5,3.5))
# NOTE: use rmarkdown::render("INLA_summary.Rmd") to avoid having to reload Zuur.RData
```

## Introduction

This is based on Chapter 24 of Volume II of _Beginner's Guide to Spatial, Temporal and Spatial-Temporal Ecological Data Analysis with R-INLA. Section numbers refer to section numbers in Chapter 24 for convenience to me, which are:

* 24.1 Areal vs geostatistical data
* 24.2 Neighbourhood data
* 24.3 Tornado data used throughout chapter
* 24.4 Model specification
* 24.5 CAR correlation
* 24.6 GAM with iCAR (though actually a GLM with iCAR)
* 24.7 Zero-inflated GAM/GLM with iCAR
* 24.8 Changing the priors, and BYM2 model
* 24.9 Spatio-temporal analysis and display

Some of the above sections are not relevant to us analytically, but I'll put them in where Zuur has done useful visualisations. Likewise some sections use GAMs rather than GLMs and I'll show the former, but we can easily adapt for the latter. I won't show most of the code, but have access to it. Some of the graphs/maps in this Word document are a bit squished as I've banged it together in RMarkdown and haven't got the settings right.

**What this document contains**

* a run through all the different sections of Zuur's Chapter 24.
* take-home messages for what we should do with the Covid19 stuff. 

```{r setup_libraries_data}
library(raster)
library(rgdal)
library(sp)
library(spdep)
library(INLA)
library(mgcv)
library(ggplot2)
source("HighstatLibV11.R")
# load("Zuur.RData") # Don't run unless essential as 1Gb file
```

## 24.1 Areal vs geostatistical data
We know all this. Zuur explains how common areal is in medical sciences

## 24.2 Neighbourhood data
Introduces the Illinois county structure which looks like

```{r illinois_counties}
par(mfrow=c(1,2))
plot(Illi.shp)
plot(Illi.shp, border=grey(0.5))  
plot(Illi.nb, 
     coords = Coords, 
     add = TRUE,
     pch = 16,
     lwd = 2)
par(mfrow=c(1,1))
```

## 24.3 Tornado data
This is 4,386 observations (102 counties in 43 years) with the percentage of observations equal to zero at 83%. Elevation per county, time (years),  area and population of each county available. No obvious patterns but this is across all counties, so fairly coarse.

```{r covariate_scatterplot}
MyX <- c("LogDensity", "elevS", "area", "Year")
MyMultipanel.ggp2(Z = Torn,
                  varx = MyX,
                  vary = "nT",
                  ylab = "Number of tornados",
                  addSmoother = TRUE,
                  addRegressionLine = FALSE,
                  addHorizontalLine = FALSE) 
```

## 24.4 Model specification
Zuur says the max no. of tornados in any county is too low to justify negative binomial. ZIP or Poisson GLM could be used. Basic model of former is:

$$T_{is}\sim Poisson(\mu_{is})$$
$$E(T_{is})=\mu_{is}=var(T_{is})$$
$$log(\mu_{is})=Intercept +Covariates_{is}+County_i$$
$T_{is}$ is no.s of tornados in county _i_ in year _s_. In original paper `rw` added as smoother for year.

## 24.5 CAR correlation
### 24.5.1 Random effects
This now uses County more formally as a random effect. To conform with most previous literature Zuur changes the County component to $u_i$ where $i$ is 1 to 102 for no. of counties:
$$log(\mu_{is})=Intercept +Covariates_{is}+u_i$$
which is a conventional random-effects model.

### 24.5.2 CAR mathematics
This section explains CAR and intrinsic CAR (iCAR), and underlying mathematics. Hopefully we can skip for now.

## 24.6 GAM with iCAR on tornado data
Steve and I (and suspect Ellen) don't like GAMs, but there are some useful diagnostics etc. And the subtitles relate to GLMs, and they look like GLMs to me!

### 24.6.1 Covariates in the link function
Gives the log-link function as
$$log(\mu_{is})=\beta_1 + \beta_2\cdot elevS_i + \beta_3\cdot LogDens_{is} + \beta_4\cdot Area_i + f(Year_s) + u_i$$
Here $f(Year_s)$ is a spline smoothing function (which I can't find in the code, so maybe he just used GLM after all!). $u_i$ is the spatially correlated random effects $u_i\sim N(0, \sigma^2_{CAR})$

### 24.6.2 Running the Poisson GLM with iCAR
He now switches back to comparing a standard Poisson GLM with and without iCAR, done via the `model = "besag"` option in INLA. The Besag model has lower DIC and WAIC.

### 24.6.3 Covariate effects
Here he shows the results for the Besag Poisson GLM (iCAR). Note that he standardised the covariates:

```{r}
head(round(M2$summary.fixed[, c("mean", "0.025quant", "0.975quant")], 3),4)
```

and changes over time. This graph shows posterior mean values and 95% credible intervals. The solid line is from a smoother $f(Year_s)$ and the vertical ticks indicate years for which data available:

```{r}
Ns <- nrow(Torn)
f.Year    <- M2$summary.lincomb.derived[1:Ns + 0 * Ns, "mean"] 
SeLo.Year <- M2$summary.lincomb.derived[1:Ns + 0 * Ns,"0.025quant"] 
SeUp.Year <- M2$summary.lincomb.derived[1:Ns + 0 * Ns,"0.975quant"]

IYear <- order(Torn$Year)

MyData <- data.frame(
  mu   = c( f.Year[IYear]), 
  SeUp = c(SeUp.Year[IYear]), 
  SeLo = c(SeLo.Year[IYear]), 
  Xaxis = c(sort(Torn$Year)),
  ID    = factor(rep(c("Year smoother"), each = nrow(Torn))))


library(ggplot2)
p <- ggplot()
p <- p + xlab("Year") + ylab("Smoother")
p <- p + theme(text = element_text(size = 15))
p <- p + geom_line(data = MyData, 
                   aes(x = Xaxis, y = mu))

p <- p + geom_ribbon(data = MyData, 
                     aes(x = Xaxis, 
                         ymax = SeUp, 
                         ymin = SeLo),
                     alpha = 0.6)

my.ggp.yrange <- c(0, 0)
XPos <- c(Torn$Year)
XID  <- rep(c("Year smoother"), each = nrow(Torn) )

MyData2 <- data.frame(Y     = rep(my.ggp.yrange, each = nrow(Torn)),
                      Xaxis = XPos,
                      ID    = factor(XID))
p <- p + geom_text(data = MyData2,
                   aes(y = Y,
                       x = Xaxis,
                       label = "|"),
                   size = 1)
p

```

### 24.6.4 Spatial random effects (Now things start to get interesting)
In this map shows the numbers in each county represent the % change change in tornados due to the spatial random effect $u_i$ (blue decrease, red increase). Counties with a thick border are "important" in that their 95% credible intervals do not include zero.

```{r fig_24_08}
Illi.shp <- readOGR("Shapefiles/Tornado_Illinois.shp")
Illi.shp$u.pm <- M2$summary.random$ID$mean

u.pm <- M2$summary.random$ID$mean
Id  <- 1:102





# Plot the spatial random effect
Illi.shp$RandomMean <- M2$summary.random$ID$mean
Illi.shp$RandomQ50  <- M2$summary.random$ID$'0.5quant'
Illi.shp$sd         <- M2$summary.random$ID$sd



library(RColorBrewer)
crq = brewer.pal(5, "GnBu")
range(Illi.shp$RandomMean)
rng = seq(-4, 3, 1)
# spplot(Illi.shp, "RandomMean", col = "white", at = rng, 
#        col.regions = crq,
#        colorkey = list(space = "bottom", labels = paste(rng)),
#        par.settings = list(axis.line = list(col = NA)),
#        sub = "Spatial random effects")



df <- M2$summary.random$ID
names(df) = c("ID", "mean", "sd", "QL", "QM", "QH", "mode", "kld")
df$QL = (exp(df$QL) - 1) * 100
df$QM = (exp(df$QM) - 1) * 100
df$QH = (exp(df$QH) - 1) * 100
df$Sig = sign(df$QL) == sign(df$QH)
table(df$Sig)
Illi.shp$Sig = df$Sig







ggplotSpdata <- function(x, field, tfun = function(x) I(x), 
                         color = muted("yellow", l = 50, c = 80)){
  CRS0 = CRS("+proj=longlat +datum=WGS84")
  bc = getBordersAndCenters(x, CRS0)
  bc$centers$newfield = tfun(x[[field]])
  borders.df = merge(bc$borders, bc$centers[c("county", "newfield")], by = "county")
  mapO = ggplot(borders.df, aes(x = long, y = lat, group = group, fill = newfield)) +
    geom_polygon(colour = "gray", size = .2) +
    coord_map(project = "polyconic") + 
    theme(panel.grid.minor = element_blank(), 
          panel.grid.major = element_blank(), 
          panel.background = element_blank(),
          axis.ticks = element_blank(),
          axis.text = element_blank(),
          legend.position = "bottom") +
    xlab("") + ylab("") +
    geom_text(data = bc$centers, 
              aes(x = long, y = lat, group = NULL, label = newfield), 
              vjust = .5, size = 3, color = color)
  mapO
}


slibrary <- function(...) suppressMessages(library(...))

# This file was taken from the support material from Jagger et al. (2015).
source("CountyStatisticsSupport.R")

MygetBordersAndCenters <- function(x,CRS0){  
  centers.sp = rgeos::gCentroid(x, byid=TRUE)
  centers.sp = spTransform(centers.sp, CRS0)
  centers.df = as.data.frame(coordinates(centers.sp)) 
  names(centers.df) = c("long", "lat")
  centers.df$county = x$county  #rownames(centers.df)
  borders.df = fortify(spTransform(x,CRS0),region="county")
  borders.df$county = borders.df$id
  return(list(borders=borders.df,centers=centers.df))
}

CRS0 = CRS("+proj=longlat +datum=WGS84")
bc   = MygetBordersAndCenters(Illi.shp, CRS0)





df2  = bc$centers
df3  = cbind(df, df2)
names(df3)[5] = 'newfield'
borders.df = merge(bc$borders, 
                   df3[c("county", "newfield", "Sig")], 
                   by = "county")
borders2.df = borders.df[borders.df$Sig, ]



MyggplotSpdata <- function(x,field, tfun=function(x) I(x), color=muted("yellow",l=50,c=80)){
  CRS0=CRS("+proj=longlat +datum=WGS84")
  bc = MygetBordersAndCenters(x,CRS0)
  bc$centers$newfield = tfun(x[[field]])
  borders.df = merge(bc$borders, bc$centers[c("county","newfield")], by="county")
  mapO = ggplot(borders.df, aes(x=long, y=lat, group=group, fill = newfield)) +
    geom_polygon(colour = "gray", size = .2) +
    coord_map(project="polyconic") + 
    theme(panel.grid.minor=element_blank(), 
          panel.grid.major=element_blank(), 
          panel.background=element_blank(),
          axis.ticks = element_blank(),
          axis.text = element_blank(),
          legend.position="none") +
    xlab("") + ylab("") +
    scale_fill_gradient2() +
    geom_text(data=bc$centers, aes(x = long, y = lat, group=NULL, label = newfield), 
              vjust = .5, size = 2, color = color)
  mapO
}



p <- MyggplotSpdata(Illi.shp, "RandomMean", tfun = function(x) round((exp(x) - 1) * 100))
p <- p + scale_fill_gradient2( 
  low = muted("blue"), 
  mid = "white", 
  high = muted("red"),
  midpoint = 0, 
  space = "Lab", 
  na.value = "grey50", 
  guide = "colourbar",
  aesthetics = "fill") 

#p
p <- p + geom_polygon(data = borders2.df[borders2.df$newfield > 0, ],
                      aes(x = long, y = lat, group = group, fill = newfield),
                      color = muted("red"), size = 2) 
#p

p <- p +  geom_polygon(data = borders2.df[borders2.df$newfield < 0, ],
                       aes(x = long, y = lat, group = group, fill = newfield),
                       color = muted("blue"), size = 2) 
p <- p+
  geom_text(data = df3[df3$Sig,], 
            aes(x = long, y = lat, group = NULL, label = round(newfield)), 
            vjust = .5, size = 2, color = 'black')
p

```

If preferred, it is straightforward to display the actual spatial random effects, rather than converting into % as done here.

## 24.6.5 Model validation
The dataset has 83% zeros, but a Poisson GLM has been used. There is a clue that the model might not be right in the basic model validation plots, especially observed vs fitted values (top right) below, as fitted only goes to max of 2.

```{r fig_24_10}
mu2      <- M2$summary.fitted.values[,"mean"]
E2       <- (Torn$nT - mu2) / sqrt(mu2)

# Figure 24.10
par(mfrow = c(2,2), mar = c(5,5,2,2), cex.lab = 1.5)
plot(x = mu2, 
     y = E2,
     xlab = "Fitted values",
     ylab = "Pearson residuals")
abline(h = 0, lty = 2)

plot(x = mu2, 
     y = Torn$nT,
     xlab = "Fitted values",
     ylab = "Observed nT ",
     xlim = c(0, 12),
     ylim = c(0, 12))


plot(x = Torn$Area.std, 
     y = E2,
     xlab = "Area",
     ylab = "Pearson residuals")
abline(h = 0, lty = 2)


plot(x = Torn$LogDensity, 
     y = E2,
     xlab = "Log density",
     ylab = "Pearson residuals")
abline(h = 0, lty = 2)
```

To check exactly what is going on, Zuur draws 10,000 sets of regression parameters and random effects (eek, slow, perhaps I should do 1,000). This then creates simulated count data, which can be compared with the original. In the graph below the top panel shows the number of zeros in each of the 10,000 simulated datasets; the red dot is the actual observed data. The bottom panel are the Pearson residuals (red dot observed). In both cases the model has performed **poorly**.

```{r fig_24_11}
par(mfrow = c(2,1), mar = c(5,5,2,2), cex.lab = 1.5)

Z <- table(zeros)
Range <- range(as.numeric(names(Z)))
SumZeros <- sum(Torn$nT == 0)
x1 <- min(Range[1], SumZeros)
x2 <- max(Range[2], SumZeros)

plot(table(zeros), 
     xlab = "How often do we have 0, 1, 2, 3, etc. number of zeros",
     ylab = "Frequency",
     xlim = c(0.98 * x1, 1.02 * x2),
     main = "")
points(x = sum(Torn$nT == 0), 
       y = 0, 
       pch = 16, 
       cex = 3, 
       col = 2)
#The red dot is the number of zeros in the original data set.
#The data simulated from the Poisson model
#         does not contain enough zeros.


Range <- range(SSsim)
SSE <- sum(E2^2) / N
x1 <- min(Range[1], SSE)
x2 <- max(Range[2], SSE)

hist(SSsim, 
     xlim = c(0.98 * x1, 1.02 * x2),
     main = "", 
     xlab = "Sum of squared Pearson residuals / N")

points(x = sum(E2^2) / N, 
       y = 0, 
       pch = 16, 
       cex = 3, 
       col = 2)
```

## 24.7 Zero-inflated GAM/GLM with iCAR
Zuur then repeats the analysis with a ZIP. The key table is below; could do model validation as above (not shown as my PC will blow-up).

```{r ZIP_table_24_7}
dic  <- c(M1$dic$dic, M2$dic$dic,  M3$dic$dic)   
waic <- c(M1$waic$waic, M2$waic$waic, M3$waic$waic)
Z.out     <- cbind(dic, waic)
rownames(Z.out) <- c("Poisson GAM",  
                     "Poisson GAM + Besag",
                     "ZIP GAM + Besag")
Z.out
```

## 24.8 Changing the priors and the BYM2 model
Suspect you know most of this stuff already though I'm only getting to grips with it, so I'll skim over what this contains. Note that Zuur shows the examples with the Poisson model, not the ZIP model, simply because most of previous graphs / maps were on Poisson model. Of course, it would in reality be better to use ZIP with his data.

### 24.8.1 Hyperparamter of iCAR
Zuur notes that the `summary(model)` gives loads of output that includes the hyperparmater:

```{r hyperparam}
M2$summary.hyperpar
```

This value is the precision $\tau_{CAR}$ which needs to be converted into into the standard deviation $\sigma_{CAR}$

```{r tau_sigma, echo = TRUE}
tau_CAR <- M2$marginals.hyperpar$`Precision for ID`
sigma_CAR <- inla.emarginal(function(x) (1/sqrt(x)), tau_CAR)
sigma_CAR
```

### 24.8.2 Penalised complexity prior
OK, I'll admit that I don't understand PC priors though notice you have them in your code (ahem!).

### 24.8.3 BYM model for spatial correlation
Zuur now formally introduces the BYM model. Our original model was

$$log(\mu_{is})=Intercept +Covariates_{is}+u_i$$
where $u_i$ are the spatially correlated random effects, calculated by averaging neighbouring random effects. There are 102 counties in Illinois (equivalent to our Teesside postcodes), giving 102 $u_i$'s. **But**, if there is extra variation due to postcodes, and if this extra variation is **not** spatially correlated, the problem with iCAR is it can't separate between pure spatial correlation between counties and extra random variation at counties.

The BYM solution is to include an extra random effect $v_i$ that is independent, identical and normally distributed (iid):

$$log(\mu_{is})=Intercept +Covariates_{is}+u_i + v_i$$
As we now have this extra term, we end up with both $\sigma_{CAR}$ and $\sigma_v$.

Zuur then issues a **warning about ZIP** in BYM. If I've understood the text correctly, if some non-neighbouring counties have loads of zeros, the $v_i$ terms will try and model this explicitly, causing the ZIP to blow a wobbly. So he sticks to Poisson, expressed as:

```{r eval = FALSE, echo = TRUE}
HyperBYM <- list(
  prec.unstruct=list(prior = "pc.prec",param = c(1, 0.5)),
  prec.spatial=list(prior = "pc.prec",param = c(1, 0.5)))

f4 <- nT ~ ElevS.std + LogDensity.std + Area.std + Year.cr +
  f(ID, 
    model = "bym", 
    graph = Illi.adj,
    hyper = HyperBYM, 
    adjust.for.con.comp = FALSE,
    constr = TRUE,
    scale.model=TRUE)

M4 <- inla(formula = f4, 
              lincomb = lcs.Year,
              family = "poisson", 
              data = Torn,
              control.compute = list(dic = TRUE, waic = TRUE))
```

again he produces a table for the GAM/GLM results, with the BYM lowest WAIC.

```{r}
# And compare the models with DICs and WAICs
dic  <- c(M1$dic$dic, M2$dic$dic,  M4$dic$dic)   
waic <- c(M1$waic$waic, M2$waic$waic,  M4$dic$dic)
Z.out     <- cbind(dic, waic)
rownames(Z.out) <- c("Poisson GAM",  
                     "Poisson GAM + Besag",
                     "Poisson GAM + BYM")
Z.out
```

Zuur also states that it could be configured as a Besag model with a separate iid term which I think I spotted in one bit of Steve's code.

###  24.8.4 BYM2 model for spatial correlation
Introduced by Riebler et al. (2016) and described as now "the standard model for spatially correlated aerial data at the time of writing this book". Whilst BYM has separate spatial and noise terms, they often interact, making it very difficult to specify good priors for $\sigma_u$ and $sigma_v$. The model BYM2 is

$$log(\mu_{is})=Intercept + Covariates_{is}+\sigma \cdot (\sqrt {1-\phi}\cdot v_i+\sqrt{\phi} \cdot u_i$$
where

* $u_i$ = spatially correlated noise
* $v_i$ = pure noise
* $\phi$ varies from 0 to 1. If $\phi$ = 0 then only pure noise, no spatial correlation; $\phi$ = 1 then only spatial correlation and no pure noise. If $\phi$ is >0 and <1 then both terms play a combined role.
* $\sigma$ = controls how large or small this combined role is.

The main advantage of BYM2 is that with separate $\sigma$ and $\phi$ terms interpretation is much easier, and it is also makes it simpler to specify the PC terms.

## 24.9 Spatio-temporal correlation for aerial data (useful for Teesside)
This section is very useful, and aligns closely with what you've already done for Teesside. In particular, it uses Poisson BYM with `rw2` for time. It also shows how the 4 different types of possible interaction models can be built up, which would allow us to do the sort of presentation in the Zika/Dengue fever paper.

### 24.9.1 Space and time
The basic model is:

$$log(\mu_{is})=Intercept + Covariates_{is} + Spatial_i + Temporal_s$$

where $i$ is counties (postcodes for us) and $s$ is years (weeks in 2020 for us). Of course, in sections 24.1 to 24.8 there hasn't actually been an explicit $Temporal$ term as it was subsumed into a GAM smoother. Finally Zuur gets rid of that, and does something more conventional:

* spatial modelled by BYM (which includes spatially correlated and noise iid)
* temporal modelled by `rw2` smoother for year
* also add a temporal iid for a year random effect (week for Teesside)

This gives a model of:

$$log(\mu_{is})=Intercept + Covariates_{is} + u_i + v_i + \gamma_s + \phi_s$$
where we now have 4 types of random effects:

* $u_i$ BYM spatial correlated effect
* $v_i$ BYM spatial noise iid
* $\gamma_s$ temporal long-term smoother
* $\phi_s$ temporal noise iid

Interpretation of the $\gamma_s + \phi_s$ is that there is a long-term smoother from the `rw2` ($\gamma_s$) and each year, or week for Teesside, may have random variation from this long-term trend ($\phi_s$).

This diagram helps visualise the main terms and the 4 random effects:

```{r main_terms_plus_rnd_effects, fig.dim=c(4.5,4.5)}
par(mfrow=c(1,1)) # Too cramp to read in RStudio if on one plot
plot(x = 0, y = 0, axes = FALSE, type = "n",
     xlim = c(0, 11),
     ylim = c(0,11),
     xlab = "",
     ylab = "")

lines(x=c(3,3),y=c(1,10))
lines(x=c(1,10),y=c(3,3))
xy.Ind <- expand.grid(4:9, seq(4,9, by = 0.5))
points(xy.Ind, pch = 16)
text(x=10, y = 3.5, "Time", cex = 1.5)
text(x=3, y = 10.5, "Space", cex = 1.5)

# At the bottom:
points(x = 4:9, y = rep(1,6), pch = 1, cex = 2.5)
text(10, 1, expression(italic(phi[s])), cex = 1.5)
text(10, 2, expression(italic(gamma[s])), cex = 1.5)

rect(xleft = 3.75, ybottom = 1.75, xright = 9.25, ytop = 2.25,
     col = grey(0.5))

# left
points(y = seq(4,9, by = 0.5), x = rep(1,11), pch = 1, cex = 2)
text(1, 10, expression(italic(v[i])), cex = 1.5, font = 3)
text(2, 10, expression(italic(u[i])), cex = 1.5)

rect(xleft = 1.75, ybottom = 3.75, ytop = 9.25, xright = 2.25,
     col = grey(0.5))
```

Here the grey rectangles represent spatially correlated random effects and temporally correlated random effects. The open circles represent iid for space and time. The black dots represent observations.

This is in R the structure of the model, which I think matches the one you're currently using for Teesside.

```{r spatio_temporal model, echo=TRUE, eval=FALSE}
Torn$County <- Torn$ID
Torn$County1 <- Torn$ID
Torn$Year0 <- Torn$Year - min(Torn$Year) + 1
Torn$Year1 <- Torn$Year - min(Torn$Year) + 1

Hyper <- list(theta = list(prior = "pc.prec", 
                           param = c(0.01, 0.0001)))

f7 <- nT ~ ElevS.std + LogDensity.std + Area.std + 
           f(County, 
             model = "bym", 
             graph = Illi.adj,
             adjust.for.con.comp = FALSE,
             constr = TRUE,
             scale.model = TRUE) +
           f(Year, 
             model = "rw2") +
           f(Year1,
             model = "iid")

lcs <- inla.make.lincombs(Year = diag(43),  Year1 = diag(43))


M7 <- inla(formula = f7, 
           family = "poisson", 
           lincomb=lcs,
           data = Torn,
           control.compute = list(dic = TRUE, waic = TRUE))
```

Spatial random effects can be shown as a nice map as in Section 24.6.4 above. We can also show the `rw2` and `iid` compnents for time as:

```{r temporal_plot}
f.Year    <- M7$summary.random$Year[, "mean"] 
SeLo.Year <- M7$summary.random$Year[,"0.025quant"] 
SeUp.Year <- M7$summary.random$Year[,"0.975quant"]

f.Yeariid    <- M7$summary.random$Year1[, "mean"] 
SeLo.Yeariid <- M7$summary.random$Year1[,"0.025quant"] 
SeUp.Yeariid <- M7$summary.random$Year1[,"0.975quant"]


# Combine the smoother, and 95% CIs
MyData <- data.frame(mu   = c(f.Year),
                     SeUp = c(SeUp.Year),
                     SeLo = c(SeLo.Year),
                     Xaxis = c(M7$summary.random$Year[,1]))

MyData.iid <- data.frame(mu   = c(f.Yeariid),
                        SeUp = c(SeUp.Yeariid),
                        SeLo = c(SeLo.Yeariid),
                        Xaxis = c(M7$summary.random$Year1[,1]+1969))

# Figure 24.13
p <- ggplot()
p <- p + xlab("Time") + ylab("Temporal trend and iid noise")
p <- p + theme(text = element_text(size = 15))
p <- p + geom_line(data = MyData, 
                   aes(x = Xaxis, y = mu),
                   lwd = 3)

p <- p + geom_ribbon(data = MyData, 
                     aes(x = Xaxis, 
                         ymax = SeUp, 
                         ymin = SeLo),
                     alpha = 0.6)

p <- p + geom_line(data = MyData.iid, 
                   aes(x = Xaxis, y = mu))
p
```

The solid thick line is `rw2` and trend $\gamma_s$, the thin line is `iid` noise $\phi_s$ and the grey bands are 95% credible intervals.

### 24.9.2 Four types of interactions
The model in Section 24.9.1 assumes the spatial and temporal effects are independent and do not interact in any way. i.e. it assumes that spatial effects are constant over time, and temporal effects are constant over space. This is unlikely to be true for Teesside. So we can rewrite the model as:

$$log(\mu_{is})=Intercept + Covariates_{is} + Spatial_i + Temporal_s + Spatial_i \times Temporal_s$$

Given that both spatial and temporal are actually two components, this can be re-written as:

$$log(\mu_{is})=Intercept + Covariates_{is} + u_i + v_i + \gamma_s + \phi_s + \delta_{is}$$

where $\delta_{is}$ is the spatial-temporal interaction term. Knorr-Held (2000) define four types of interaction depending on how the spatial and temporal interaction cross-products are combined. They each provide different information about the underlying patterns in the data:

* **Type I**   spatial iid $v_i$ and temporal iid $\phi_s$
* **Type II**  spatial iid $v_i$ and temporal trend $\gamma_s$
* **Type III** spatially correlated $u_i$ and temporal iid $\phi_s$
* **Type IV**  spatially correlated $u_i$ and temporal trend $\gamma_s$

### 24.9.3 Type I Interaction
Since this is based on both the spatial and temporal iid's, this is best thought of as an observation level random intercept rather than a "real" interaction. It may indicate an unmeasured covariate that has an effect that is not structured in time and space. However it is not good for zero-inflated data. We can show it in our schematic diagram below, with the open circles around each observation indicating the interaction iid noise:

```{r type_I, fig.dim=c(4.5,4.5)}
par(mfrow=c(1,1)) # Too cramp to read in RStudio if on one plot
plot(x = 0, y = 0, axes = FALSE, type = "n",
     xlim = c(0, 11),
     ylim = c(0,11),
     xlab = "",
     ylab = "")

lines(x=c(3,3),y=c(1,10))
lines(x=c(1,10),y=c(3,3))
xy.Ind <- expand.grid(4:9, seq(4,9, by = 0.5))
points(xy.Ind, pch = 16)
text(x=10, y = 3.5, "Time", cex = 1.5)
text(x=3, y = 10.5, "Space", cex = 1.5)

# At the bottom:
points(x = 4:9, y = rep(1,6), pch = 1, cex = 2.5)
text(10, 1, expression(italic(phi[s])), cex = 1.5)
text(10, 2, expression(italic(gamma[s])), cex = 1.5)

rect(xleft = 3.75, ybottom = 1.75, xright = 9.25, ytop = 2.25,
     col = grey(0.5))

# left
points(y = seq(4,9, by = 0.5), x = rep(1,11), pch = 1, cex = 2)
text(1, 10, expression(italic(v[i])), cex = 1.5, font = 3)
text(2, 10, expression(italic(u[i])), cex = 1.5)

rect(xleft = 1.75, ybottom = 3.75, ytop = 9.25, xright = 2.25,
     col = grey(0.5))
points(xy.Ind, pch = 1, cex = 2)
```

The formula for this type of model is:

```{r echo=TRUE, eval=FALSE}
Torn$County.Year <- as.numeric(as.factor(paste(Torn$County, Torn$Year, sep = ".")))

f8 <- nT ~ ElevS.std + LogDensity.std + Area.std + 
  f(County, 
    model = "bym", 
    graph = Illi.adj,
    adjust.for.con.comp = FALSE,
    constr = TRUE,
    scale.model = TRUE) +
  f(Year, 
    model = "rw2") +
  f(Year1,
    model = "iid") +
  f(County.Year, model = "iid")
```

### 24.9.4 Type II Interaction (starts to be more interesting)
This is the spatial iid $v_i$ and temporal trend $\gamma_s$. Now each county (postcode) is allowed to have its own `rw2` trend, and these are deviations from the overall smoother $\gamma_s$. Temporal trends are different per county, but they do not have any structure in space. This model is most useful if there are unmeasured covariates that cause the temporal trend in a specific county (postcode) to be different from the main trend.

We can show this schematically below, where the light grey rectangles are plotted for each county (postcode) along the time axis, and represent deviations from the main term $Temporal_s$.


```{r type_II, fig.dim=c(4.5,4.5)}
par(mfrow=c(1,1)) # Too cramp to read in RStudio if on one plot
plot(x = 0, y = 0, axes = FALSE, type = "n",
     xlim = c(0, 11),
     ylim = c(0,11),
     xlab = "",
     ylab = "")

lines(x=c(3,3),y=c(1,10))
lines(x=c(1,10),y=c(3,3))
xy.Ind <- expand.grid(4:9, seq(4,9, by = 0.5))
points(xy.Ind, pch = 16)
text(x=10, y = 3.5, "Time", cex = 1.5)
text(x=3, y = 10.5, "Space", cex = 1.5)

# At the bottom:
points(x = 4:9, y = rep(1,6), pch = 1, cex = 2.5)
text(10, 1, expression(italic(phi[s])), cex = 1.5)
text(10, 2, expression(italic(gamma[s])), cex = 1.5)

rect(xleft = 3.75, ybottom = 1.75, xright = 9.25, ytop = 2.25,
     col = grey(0.5))

# left
points(y = seq(4,9, by = 0.5), x = rep(1,11), pch = 1, cex = 2)
text(1, 10, expression(italic(v[i])), cex = 1.5, font = 3)
text(2, 10, expression(italic(u[i])), cex = 1.5)

rect(xleft = 1.75, ybottom = 3.75, ytop = 9.25, xright = 2.25,
     col = grey(0.5))

for (i in 1:11){
 rect(xleft = 3.5, ybottom = 3.9 +0.5*(i-1), ytop = 4.1+0.5*(i-1), xright = 9.5,
      col = grey(0.7))
}
points(xy.Ind, pch = 16)

```

The R code to do this is Type II model is:

```{r, echo=TRUE, eval=FALSE}
f9 <- nT ~ ElevS.std + LogDensity.std + Area.std + 
  f(County, 
    model = "bym", 
    graph = Illi.adj,
    adjust.for.con.comp = FALSE,
    constr = TRUE,
    scale.model = TRUE) +
  f(Year0, 
    model = "rw2") +
  f(Year1,
    model = "iid") +
  f(County1, 
    model = "iid",
    group = Year1,
    control.group = list(model = "rw2"))
```

where the bit that begins `f(County1,...` defines the interaction, and give precisions for the various components of:

```{r}
round(M9$summary.hyperpar[,c("mean", "mode")], 2)
```

In the above table, the first two are the $Spatial_i$ term, the third is the long-term trend $\gamma_s$, the fourth is the iid temporal $\phi_s$ and the fifth is the interaction.

The interaction terms are deviations from the long-term for each of the 102 counties (=Teesside postcodes), shown as:

```{r deviations_from_lt_trend}
Time <- M9$summary.random$Year0[,"ID"] + 1969
MainTr <- M9$summary.random$Year0[,"mean"]
TimeComplete <- rep(1970:2013, each = 102)

ST.II <- M9$summary.random$County1
ST.II$TimeComplete <- TimeComplete

ST.IIa <- subset(ST.II, TimeComplete != 2007)

ST.IIa$Time <- rep(Time, each = 102)
ST.IIa$MainTrend <- rep(MainTr, each = 102)
RE.year <- M9$summary.random$Year0[,"mean"]
ST.IIa$RE.year <- rep(RE.year, each = 102)

# Interaction trend area 1

library(ggplot2)
# Figure 24.16 and 24.17 (drop the exp for Figure 24.16)
p <- ggplot()
p <- p + xlab("Time") + ylab("Interaction smoother for each county")
p <- p + theme(text = element_text(size = 15), legend.position="none")
p <- p + geom_line(data = ST.IIa, col =1,
                   # aes(x = Time, y = exp(mean + MainTrend + RE.year), group = ID))
                   aes(x = Time, y = mean + MainTrend + RE.year, group = ID))
p

```

Perhaps more useful is to look at the long-term trend, temporal iid and space-time interaction, with the exponential component (since it was a Poisson model). If the line is above 1 then there is an increase in the expected number of tornados due to time and the space x time interaction (and _vice versa_). You can see that there is actually a reduction in the expected number of tornados than expected between 1980 and 2000

```{r}
Time <- M9$summary.random$Year0[,"ID"] + 1969
MainTr <- M9$summary.random$Year0[,"mean"]
TimeComplete <- rep(1970:2013, each = 102)

ST.II <- M9$summary.random$County1
ST.II$TimeComplete <- TimeComplete

ST.IIa <- subset(ST.II, TimeComplete != 2007)

ST.IIa$Time <- rep(Time, each = 102)
ST.IIa$MainTrend <- rep(MainTr, each = 102)
RE.year <- M9$summary.random$Year0[,"mean"]
ST.IIa$RE.year <- rep(RE.year, each = 102)

# Interaction trend area 1

library(ggplot2)
# Figure 24.16 and 24.17 (drop the exp for Figure 24.16)
p <- ggplot()
p <- p + xlab("Time") + ylab("Time + space-time interaction for each county")
p <- p + theme(text = element_text(size = 15), legend.position="none")
p <- p + geom_line(data = ST.IIa, col =1,
                   aes(x = Time, y = exp(mean + MainTrend + RE.year), group = ID))
p
```

**Note** we could also present the effects of the interaction as spatial maps, which we will do in the next section on Type III.

### 24.9.5 Type III Interaction
This is like Type II, but the other way round in that the focus is primarily on space. There is the usual overall spatial pattern modelled with BYM ($Spatial_i$). For each year (week for Teesside) there is a spatial deviation from the main spatial pattern. These spatial deviations are more similar for counties (postcodes) close to one another. But these deviations do not persist in time.

Examples - a one of waste spill into a river might affect the macroinverts in neighbouring streams, but only for a short period of time e.g. one week. A wildfire might affect chances of tornados in neighbouring counties, but only in one year.

Output from the model is the main trend $Spatial_i$ from the BYM, a main trend $Temporal_s$ from the `rw2` + `iid`, and a spatial x temporal interaction. The latter can be shown as separate maps, one for each year, showing the spatial deviation from the main term $Spatial_i$.

Using the same sort of schematic as before, we can visualise Type III model as:

```{r type_III, fig.dim=c(4.5,4.5)}
par(mfrow=c(1,1)) # Too cramp to read in RStudio if on one plot
plot(x = 0, y = 0, axes = FALSE, type = "n",
     xlim = c(0, 11),
     ylim = c(0,11),
     xlab = "",
     ylab = "")

lines(x=c(3,3),y=c(1,10))
lines(x=c(1,10),y=c(3,3))
xy.Ind <- expand.grid(4:9, seq(4,9, by = 0.5))
points(xy.Ind, pch = 16)
text(x=10, y = 3.5, "Time", cex = 1.5)
text(x=3, y = 10.5, "Space", cex = 1.5)

# At the bottom:
points(x = 4:9, y = rep(1,6), pch = 1, cex = 2.5)
text(10, 1, expression(italic(phi[s])), cex = 1.5)
text(10, 2, expression(italic(gamma[s])), cex = 1.5)

rect(xleft = 3.75, ybottom = 1.75, xright = 9.25, ytop = 2.25,
     col = grey(0.5))

# left
points(y = seq(4,9, by = 0.5), x = rep(1,11), pch = 1, cex = 2)
text(1, 10, expression(italic(v[i])), cex = 1.5, font = 3)
text(2, 10, expression(italic(u[i])), cex = 1.5)

rect(xleft = 1.75, ybottom = 3.75, ytop = 9.25, xright = 2.25,
     col = grey(0.5))

for (i in 1:6){
 rect(xleft = 3.9 +(i-1),
      ybottom = 3.9,
      ytop = 9.5,
      xright = 4.1 + (i-1),
      col = grey(0.7))
}

points(xy.Ind, pch = 16)
```

The formulation of the model in INLA is similar to that for Type II. We need another copy of `Year` as we can't use the same name multiple times in INLA, and the definition of the interaction has swapped year and county, and uses a different dependency structure of the observations from the same group, namely iCAR:

```{r echo=TRUE, eval=FALSE}
Torn$Year2 <- Torn$Year - min(Torn$Year) + 1

f10 <- nT ~ ElevS.std + LogDensity.std + Area.std + 
  f(County, 
    model = "bym", 
    graph = Illi.adj,
    adjust.for.con.comp = FALSE,
    constr = TRUE,
    scale.model = TRUE) +
  f(Year0, 
    model = "rw2") +
  f(Year1,
    model = "iid") +
  f(Year2, 
    model = "iid",
    group = County1,
    control.group = list(model = "besag", graph = Illi.adj))
```

The following summarises the results as maps of the counties for each year for the Spatial x Temporal Type III interaction. Blue is large negative, white is random effects -1 to +1, and red is large positive.

```{r type_III_maps, fig.dim=c(5.5,4.5)}
delta.intIII <- data.frame(delta = M10$summary.random$Year2[,2],
                           year  = Torn$Year,
                           ID.area = Torn$County)

delta.intIII.matrix <- matrix(delta.intIII[,1], 
                              nrow =102,
                              ncol = 43,
                              byrow = TRUE)
#delta.intIII.matrix[1:5, 1:5]
rownames(delta.intIII.matrix)<- 1:102


cutoff.interaction <- c(-2,-1, 1,2)

delta.intIII.factor <- data.frame(NAME=1:102)
for(i in 1:43){
  delta.factor.temp <- cut(delta.intIII.matrix[,i],
                          breaks = cutoff.interaction,
                          include.lowest=TRUE) 
  delta.intIII.factor <- cbind(delta.intIII.factor,delta.factor.temp)
}
colnames(delta.intIII.factor)<- c("NAME",Time)


attr(Illi.shp, "data") <- data.frame(Torn, intIII = delta.intIII.factor)
library(lattice)
trellis.par.set(axis.line=list(col=NA))

#Figure 24.19
spplot(obj=Illi.shp, zcol=c("intIII.1973", "intIII.1974", "intIII.1975",
                            "intIII.1976", "intIII.1977", "intIII.1978",
                            "intIII.1979", "intIII.1980", "intIII.1981",
                            "intIII.1982", "intIII.1983", "intIII.1984",
                            "intIII.1985", "intIII.1986", "intIII.1987",
                            "intIII.1988", "intIII.1989", "intIII.1990",
                            "intIII.1991", "intIII.1992", "intIII.1993",
                            "intIII.1994", "intIII.1995", "intIII.1996",
                            "intIII.1997", "intIII.1998", "intIII.1999",
                            "intIII.2000", "intIII.2001", "intIII.2002",
                            "intIII.2003", "intIII.2004", "intIII.2005",
                            "intIII.2006", "intIII.2008", "intIII.2009",
                            "intIII.2010", "intIII.2011", "intIII.2012",
                            "intIII.2013"), 
       col.regions=c("blue", "white", "red"), #gray(2.5:0.5/3),
       names.attr=c(seq(1973, 2006),seq(2008, 2013)),#Time,
       main="")

```

Interpretation - you can see that there are actually quite a lot of years where the predominant colour in the map is blue. This reflects the 83% zeros in the original tornado data.

Not shown - we also of course have the main spatial term, which we could show as in Section 24.5.4 via a map with different colours, thicknesses of borders. We also have the main temporal term (with iid) which we could show as in Section 24.9.1. Zuur also recommends model validation as usual, e.g. via simulation done in Section 24.6.5

### 24.9.6 Type IV Interaction
We still have the main terms $Spatial_i$ and $Temporal_s$. The Type IV interaction assumptions that deviations from the $Temporal_s$ trend at a specific county are not only correlated in time (as with Type II) but also correlated in space with neighbouring counties.

Example - going back to the waste spill in rivers example. In Type III a waste spill into a river might affect the macroinverts in neighbouring streams, but only for one week. In Type IV this effect might last several weeks due to the temporal correlation in time.

Conceptually, a Type IV model can be viewed as:

```{r type_IV, fig.dim=c(4.5,4.5)}
par(mfrow=c(1,1)) # Too cramp to read in RStudio if on one plot
plot(x = 0, y = 0, axes = FALSE, type = "n",
     xlim = c(0, 11),
     ylim = c(0,11),
     xlab = "",
     ylab = "")

lines(x=c(3,3),y=c(1,10))
lines(x=c(1,10),y=c(3,3))
xy.Ind <- expand.grid(4:9, seq(4,9, by = 0.5))
points(xy.Ind, pch = 16)
text(x=10, y = 3.5, "Time", cex = 1.5)
text(x=3, y = 10.5, "Space", cex = 1.5)

# At the bottom:
points(x = 4:9, y = rep(1,6), pch = 1, cex = 2.5)
text(10, 1, expression(italic(phi[s])), cex = 1.5)
text(10, 2, expression(italic(gamma[s])), cex = 1.5)

rect(xleft = 3.75, ybottom = 1.75, xright = 9.25, ytop = 2.25,
     col = grey(0.5))

# left
points(y = seq(4,9, by = 0.5), x = rep(1,11), pch = 1, cex = 2)
text(1, 10, expression(italic(v[i])), cex = 1.5, font = 3)
text(2, 10, expression(italic(u[i])), cex = 1.5)

rect(xleft = 1.75, ybottom = 3.75, ytop = 9.25, xright = 2.25,
     col = grey(0.5))

for (i in 1:11){
 rect(xleft = 3.5, ybottom = 3.9 +0.5*(i-1), ytop = 4.1+0.5*(i-1), xright = 9.5,
      col = grey(0.7))
}

for (i in 1:6){
 rect(xleft = 3.9 +(i-1),
      ybottom = 3.9,
      ytop = 9.5,
      xright = 4.1 + (i-1),
      col = grey(0.7))
}

points(xy.Ind, pch = 16)
```

The formula for a Type IV interaction can be given as

```{r eval=FALSE, echo=TRUE}
f11 <- nT ~ ElevS.std + LogDensity.std + Area.std + 
  f(County, 
    model = "bym", 
    graph = Illi.adj,
    adjust.for.con.comp = FALSE,
    constr = TRUE,
    scale.model = TRUE) +
  f(Year0, 
    model = "rw2") +
  f(Year1,
    model = "iid") +
  f(County1, 
    model = "besag", 
    graph = Illi.adj,
    group = Year1,
    control.group = list(model = "rw2"))
```

where it is the final `f(County1,...` part that defines the structure of the Type IV interaction. Zuur recommends presentation via Spatial random effects map (Section 24.5.4), Temporal random effects over time (Section 24.9.1) and an interaction map (Section 24.9.5).

### 24.9.7 Discussion
Zuur recommends running spatio-temporal models without interaction (24.9.1) and with the four types of interaction (Sections 24.9.3 to 24.9.6), and selecting the best model of the five using DICs and WAICs. This is basically what Martinez-Bello (2018) do in their Zika virus / Dengue fever paper. Any suitable error distribution can be used (not just Poisson as used here).

## Take-home messages for Teesside
I quite like the Martinez-Bello (2018) approach to the way they've presented their INLA results. Their paper at <https://www.mdpi.com/310848> is part of a special issue in _International Journal of Environmental Health and Public Health_ called "Spatio-temporal analysis of infectious diseases"<https://www.mdpi.com/journal/ijerph/special_issues/Infectious-Diseases-Spatio-Temporal>.

A few points to think about:

* Martinez-Bello only have two response variables (Denge and Zika). We have 8 lineages, so 9 responses if we include the 'all lineages' response. This makes it a mess to analyse and present. Can we cut it down and simplify?
* We have lots of covariates whereas Martinez-Bello only focus on the spatio-temporal stuff. This makes model simplification much tougher, especially if we have 8 or 9 responses to work with
* The Lancet Infectious Diseases has some bizarre formatting specifications. e.g you can't even put 23.5 as a number but have to change the decimal point to a mid line with an ASCII code. Last time it took me ages doing all this before Mr Grumpy-Face (John McConnell) <https://www.thelancet.com/laninf/about> rejected without review. Conversely TLID is impact factor of 24, whereas IJEHPH is IF 2.8

My feeling is not to do any model simplification with regard to the covariates, but just present the full model with all the covariates even if some prove to be irrelevant. If we run this separately for 9 responses (8 lineages, all lineages) and 5 model types (spatio-temporal no interaction, spatio-temporal interactions I to IV) we still have 45 models which is a nightmare. So I think it best to:

* run INLA only for the "all lineages" response, giving us 5 models to present. As we will be presenting all 5 versions of the spatio-temporal, this should be sensitive enough to pick up changes that are driven by an individual lineage. We can then expand on these in the discussion. With only 5 models we can if we want to a robust simulation-based model testing as we did in Section 24.6.5 above.
* if we decide to go for TLID we can have a long supplementary document with 45 models but I still don't think that's a good idea (no one will read it anyway)
* we can include the phylogenetic dendrogram, and graphs of patterns over time for each lineage separately as we already have in the draft manuscript.


