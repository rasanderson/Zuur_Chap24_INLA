---
title: "Example spatio-temporal INLA"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, fig.dim=c(4.5,3.5))
# NOTE: use rmarkdown::render("INLA_summary.Rmd") to avoid having to reload Zuur.RData
```

## Introduction

This is based on Chapter 24 of Volume II of _Beginner's Guide to Spatial, Temporal and Spatial-Temporal Ecological Data Analysis with R-INLA. Section numbers refer to section numbers in Chapter 24 for convenience to me, which are:

* 24.1 Areal vs geostatistical data
* 24.2 Neighbourhood data
* 24.3 Tornado data used throughout chapter
* 24.4 Model specification
* 24.5 CAR correlation
* 24.6 GAM with iCAR (though actually a GLM with iCAR)
* 24.7 Zero-inflated GAM/GLM with iCAR
* 24.8 Changing the priors, and BYM2 model
* 24.9 Spatio-temporal analysis and display

Some of the above sections are not relevant to us analytically, but I'll put them in where Zuur has done useful visualisations. Likewise some sections use GAMs rather than GLMs and I'll show the former, but we can easily adapt for the latter. I won't show the code, but have access to it.

```{r setup_libraries_data}
library(raster)
library(rgdal)
library(sp)
library(spdep)
library(INLA)
library(mgcv)
library(ggplot2)
source("HighstatLibV11.R")
# load("Zuur.RData") # Don't run unless essential as 1Gb file
```

## 24.1 Areal vs geostatistical data
We know all this. Zuur explains how common areal is in medical sciences

## 24.2 Neighbourhood data
Introduces the Illinois county structure which looks like

```{r illinois_counties}
par(mfrow=c(1,2))
plot(Illi.shp)
plot(Illi.shp, border=grey(0.5))  
plot(Illi.nb, 
     coords = Coords, 
     add = TRUE,
     pch = 16,
     lwd = 2)
par(mfrow=c(1,1))
```

## 24.3 Tornado data
This is 4,386 observations (102 counties in 43 years) with the percentage of observations equal to zero at 83%. Elevation per county, time (years),  area and population of each county available. No obvious patterns but this is across all counties, so fairly coarse.

```{r covariate_scatterplot}
MyX <- c("LogDensity", "elevS", "area", "Year")
MyMultipanel.ggp2(Z = Torn,
                  varx = MyX,
                  vary = "nT",
                  ylab = "Number of tornados",
                  addSmoother = TRUE,
                  addRegressionLine = FALSE,
                  addHorizontalLine = FALSE) 
```

## 24.4 Model specification
Zuur says the max no. of tornados in any county is too low to justify negative binomial. ZIP or Poisson GLM could be used. Basic model of former is:

$$T_{is}\sim Poisson(\mu_{is})$$
$$E(T_{is})=\mu_{is}=var(T_{is})$$
$$log(\mu_{is})=Intercept +Covariates_{is}+County_i$$
$T_{is}$ is no.s of tornados in county _i_ in year _s_. In original paper `rw` added as smoother for year.

## 24.5 CAR correlation
### 24.5.1 Random effects
This now uses County more formally as a random effect. To conform with most previous literature Zuur changes the County component to $u_i$ where $i$ is 1 to 102 for no. of counties:
$$log(\mu_{is})=Intercept +Covariates_{is}+u_i$$
which is a conventional random-effects model.

### 24.5.2 CAR mathematics
This section explains CAR and intrinsic CAR (iCAR), and underlying mathematics. Hopefully we can skip for now.

## 24.6 GAM with iCAR on tornado data
Steve and I (and suspect Ellen) don't like GAMs, but there are some useful diagnostics etc. And the subtitles relate to GLMs, and they look like GLMs to me!

### 24.6.1 Covariates in the link function
Gives the log-link function as
$$log(\mu_{is})=\beta_1 + \beta_2\cdot elevS_i + \beta_3\cdot LogDens_{is} + \beta_4\cdot Area_i + f(Year_s) + u_i$$
Here $f(Year_s)$ is a spline smoothing function (which I can't find in the code, so maybe he just used GLM after all!). $u_i$ is the spatially correlated random effects $u_i\sim N(0, \sigma^2_{CAR})$

### 24.6.2 Running the Poisson GLM with iCAR
He now switches back to comparing a standard Poisson GLM with and without iCAR, done via the `model = "besag"` option in INLA. The Besag model has lower DIC and WAIC.

### 24.6.3 Covariate effects
Here he shows the results for the Besag Poisson GLM (iCAR). Note that he standardised the covariates:

```{r}
head(round(M2$summary.fixed[, c("mean", "0.025quant", "0.975quant")], 3),4)
```

and changes over time. This graph shows posterior mean values and 95% credible intervals. The solid line is from a smoother $f(Year_s)$ and the vertical ticks indicate years for which data available:

```{r}
Ns <- nrow(Torn)
f.Year    <- M2$summary.lincomb.derived[1:Ns + 0 * Ns, "mean"] 
SeLo.Year <- M2$summary.lincomb.derived[1:Ns + 0 * Ns,"0.025quant"] 
SeUp.Year <- M2$summary.lincomb.derived[1:Ns + 0 * Ns,"0.975quant"]

IYear <- order(Torn$Year)

MyData <- data.frame(
  mu   = c( f.Year[IYear]), 
  SeUp = c(SeUp.Year[IYear]), 
  SeLo = c(SeLo.Year[IYear]), 
  Xaxis = c(sort(Torn$Year)),
  ID    = factor(rep(c("Year smoother"), each = nrow(Torn))))


library(ggplot2)
p <- ggplot()
p <- p + xlab("Year") + ylab("Smoother")
p <- p + theme(text = element_text(size = 15))
p <- p + geom_line(data = MyData, 
                   aes(x = Xaxis, y = mu))

p <- p + geom_ribbon(data = MyData, 
                     aes(x = Xaxis, 
                         ymax = SeUp, 
                         ymin = SeLo),
                     alpha = 0.6)

my.ggp.yrange <- c(0, 0)
XPos <- c(Torn$Year)
XID  <- rep(c("Year smoother"), each = nrow(Torn) )

MyData2 <- data.frame(Y     = rep(my.ggp.yrange, each = nrow(Torn)),
                      Xaxis = XPos,
                      ID    = factor(XID))
p <- p + geom_text(data = MyData2,
                   aes(y = Y,
                       x = Xaxis,
                       label = "|"),
                   size = 1)
p

```

### 24.6.4 Spatial random effects (Now things start to get interesting)
In this map shows the numbers in each county represent the % change change in tornados due to the spatial random effect $u_i$ (blue decrease, red increase). Counties with a thick border are "important" in that their 95% credible intervals do not include zero.

```{r fig_24_08}
Illi.shp <- readOGR("Shapefiles/Tornado_Illinois.shp")
Illi.shp$u.pm <- M2$summary.random$ID$mean

u.pm <- M2$summary.random$ID$mean
Id  <- 1:102





# Plot the spatial random effect
Illi.shp$RandomMean <- M2$summary.random$ID$mean
Illi.shp$RandomQ50  <- M2$summary.random$ID$'0.5quant'
Illi.shp$sd         <- M2$summary.random$ID$sd



library(RColorBrewer)
crq = brewer.pal(5, "GnBu")
range(Illi.shp$RandomMean)
rng = seq(-4, 3, 1)
# spplot(Illi.shp, "RandomMean", col = "white", at = rng, 
#        col.regions = crq,
#        colorkey = list(space = "bottom", labels = paste(rng)),
#        par.settings = list(axis.line = list(col = NA)),
#        sub = "Spatial random effects")



df <- M2$summary.random$ID
names(df) = c("ID", "mean", "sd", "QL", "QM", "QH", "mode", "kld")
df$QL = (exp(df$QL) - 1) * 100
df$QM = (exp(df$QM) - 1) * 100
df$QH = (exp(df$QH) - 1) * 100
df$Sig = sign(df$QL) == sign(df$QH)
table(df$Sig)
Illi.shp$Sig = df$Sig







ggplotSpdata <- function(x, field, tfun = function(x) I(x), 
                         color = muted("yellow", l = 50, c = 80)){
  CRS0 = CRS("+proj=longlat +datum=WGS84")
  bc = getBordersAndCenters(x, CRS0)
  bc$centers$newfield = tfun(x[[field]])
  borders.df = merge(bc$borders, bc$centers[c("county", "newfield")], by = "county")
  mapO = ggplot(borders.df, aes(x = long, y = lat, group = group, fill = newfield)) +
    geom_polygon(colour = "gray", size = .2) +
    coord_map(project = "polyconic") + 
    theme(panel.grid.minor = element_blank(), 
          panel.grid.major = element_blank(), 
          panel.background = element_blank(),
          axis.ticks = element_blank(),
          axis.text = element_blank(),
          legend.position = "bottom") +
    xlab("") + ylab("") +
    geom_text(data = bc$centers, 
              aes(x = long, y = lat, group = NULL, label = newfield), 
              vjust = .5, size = 3, color = color)
  mapO
}


slibrary <- function(...) suppressMessages(library(...))

# This file was taken from the support material from Jagger et al. (2015).
source("CountyStatisticsSupport.R")

MygetBordersAndCenters <- function(x,CRS0){  
  centers.sp = rgeos::gCentroid(x, byid=TRUE)
  centers.sp = spTransform(centers.sp, CRS0)
  centers.df = as.data.frame(coordinates(centers.sp)) 
  names(centers.df) = c("long", "lat")
  centers.df$county = x$county  #rownames(centers.df)
  borders.df = fortify(spTransform(x,CRS0),region="county")
  borders.df$county = borders.df$id
  return(list(borders=borders.df,centers=centers.df))
}

CRS0 = CRS("+proj=longlat +datum=WGS84")
bc   = MygetBordersAndCenters(Illi.shp, CRS0)





df2  = bc$centers
df3  = cbind(df, df2)
names(df3)[5] = 'newfield'
borders.df = merge(bc$borders, 
                   df3[c("county", "newfield", "Sig")], 
                   by = "county")
borders2.df = borders.df[borders.df$Sig, ]



MyggplotSpdata <- function(x,field, tfun=function(x) I(x), color=muted("yellow",l=50,c=80)){
  CRS0=CRS("+proj=longlat +datum=WGS84")
  bc = MygetBordersAndCenters(x,CRS0)
  bc$centers$newfield = tfun(x[[field]])
  borders.df = merge(bc$borders, bc$centers[c("county","newfield")], by="county")
  mapO = ggplot(borders.df, aes(x=long, y=lat, group=group, fill = newfield)) +
    geom_polygon(colour = "gray", size = .2) +
    coord_map(project="polyconic") + 
    theme(panel.grid.minor=element_blank(), 
          panel.grid.major=element_blank(), 
          panel.background=element_blank(),
          axis.ticks = element_blank(),
          axis.text = element_blank(),
          legend.position="none") +
    xlab("") + ylab("") +
    scale_fill_gradient2() +
    geom_text(data=bc$centers, aes(x = long, y = lat, group=NULL, label = newfield), 
              vjust = .5, size = 2, color = color)
  mapO
}



p <- MyggplotSpdata(Illi.shp, "RandomMean", tfun = function(x) round((exp(x) - 1) * 100))
p <- p + scale_fill_gradient2( 
  low = muted("blue"), 
  mid = "white", 
  high = muted("red"),
  midpoint = 0, 
  space = "Lab", 
  na.value = "grey50", 
  guide = "colourbar",
  aesthetics = "fill") 

#p
p <- p + geom_polygon(data = borders2.df[borders2.df$newfield > 0, ],
                      aes(x = long, y = lat, group = group, fill = newfield),
                      color = muted("red"), size = 2) 
#p

p <- p +  geom_polygon(data = borders2.df[borders2.df$newfield < 0, ],
                       aes(x = long, y = lat, group = group, fill = newfield),
                       color = muted("blue"), size = 2) 
p <- p+
  geom_text(data = df3[df3$Sig,], 
            aes(x = long, y = lat, group = NULL, label = round(newfield)), 
            vjust = .5, size = 2, color = 'black')
p

```

If preferred, it is straightforward to display the actual spatial random effects, rather than converting into % as done here.

## 24.6.5 Model validation
The dataset has 83% zeros, but a Poisson GLM has been used. There is a clue that the model might not be right in the basic model validation plots, especially observed vs fitted values (top right) below, as fitted only goes to max of 2.

```{r fig_24_10}
mu2      <- M2$summary.fitted.values[,"mean"]
E2       <- (Torn$nT - mu2) / sqrt(mu2)

# Figure 24.10
par(mfrow = c(2,2), mar = c(5,5,2,2), cex.lab = 1.5)
plot(x = mu2, 
     y = E2,
     xlab = "Fitted values",
     ylab = "Pearson residuals")
abline(h = 0, lty = 2)

plot(x = mu2, 
     y = Torn$nT,
     xlab = "Fitted values",
     ylab = "Observed nT ",
     xlim = c(0, 12),
     ylim = c(0, 12))


plot(x = Torn$Area.std, 
     y = E2,
     xlab = "Area",
     ylab = "Pearson residuals")
abline(h = 0, lty = 2)


plot(x = Torn$LogDensity, 
     y = E2,
     xlab = "Log density",
     ylab = "Pearson residuals")
abline(h = 0, lty = 2)
```

To check exactly what is going on, Zuur draws 10,000 sets of regression parameters and random effects (eek, slow, perhaps I should do 1,000). This then creates simulated count data, which can be compared with the original. In the graph below the top panel shows the number of zeros in each of the 10,000 simulated datasets; the red dot is the actual observed data. The bottom panel are the Pearson residuals (red dot observed). In both cases the model has performed **poorly**.

```{r fig_24_11}
par(mfrow = c(2,1), mar = c(5,5,2,2), cex.lab = 1.5)

Z <- table(zeros)
Range <- range(as.numeric(names(Z)))
SumZeros <- sum(Torn$nT == 0)
x1 <- min(Range[1], SumZeros)
x2 <- max(Range[2], SumZeros)

plot(table(zeros), 
     xlab = "How often do we have 0, 1, 2, 3, etc. number of zeros",
     ylab = "Frequency",
     xlim = c(0.98 * x1, 1.02 * x2),
     main = "")
points(x = sum(Torn$nT == 0), 
       y = 0, 
       pch = 16, 
       cex = 3, 
       col = 2)
#The red dot is the number of zeros in the original data set.
#The data simulated from the Poisson model
#         does not contain enough zeros.


Range <- range(SSsim)
SSE <- sum(E2^2) / N
x1 <- min(Range[1], SSE)
x2 <- max(Range[2], SSE)

hist(SSsim, 
     xlim = c(0.98 * x1, 1.02 * x2),
     main = "", 
     xlab = "Sum of squared Pearson residuals / N")

points(x = sum(E2^2) / N, 
       y = 0, 
       pch = 16, 
       cex = 3, 
       col = 2)
```

## 24.7 Zero-inflated GAM/GLM with iCAR
Zuur then repeats the analysis with a ZIP. The key table is below; could do model validation as above (not shown as my PC will blow-up).

```{r ZIP_table_24_7}
dic  <- c(M1$dic$dic, M2$dic$dic,  M3$dic$dic)   
waic <- c(M1$waic$waic, M2$waic$waic, M3$waic$waic)
Z.out     <- cbind(dic, waic)
rownames(Z.out) <- c("Poisson GAM",  
                     "Poisson GAM + Besag",
                     "ZIP GAM + Besag")
Z.out
```

